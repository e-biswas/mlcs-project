# -*- coding: utf-8 -*-
"""af_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nsyHth7sp289fB4Xhmug49rokRas5ty
"""

# Imports
import requests
import json
import random
import pickle
import matplotlib.pyplot as plt
import numpy as np
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import seaborn as sns
import pandas as pd

from sklearn.svm import SVC
from sklearn.metrics import classification_report

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import StackingClassifier
from sklearn.naive_bayes import GaussianNB



# Reading pickel
layers_to_timing = pickle.load(open("layers_af_timing_dict.pl", "rb"))

# To Dataframe
df = pd.DataFrame(layers_to_timing)

# Making seperate CSV for individual layers

activation_f = ["relu", "elu", "softmax", "tanh", "sigmoid"]

for l in range(1, 6):
  df_name = f"layer_{l}.csv"
  temp_df = pd.DataFrame(columns = ['cycle', 'activation'])

  for af in activation_f:
    col_name = f"{l}_{af}"
    cycles = df[col_name].to_list()
    activation = [af for _ in range(len(cycles))]
    temp = pd.DataFrame(columns = ['cycle', 'activation'])
    temp['cycle'] = cycles
    temp['activation'] = activation

    temp_df = pd.concat([temp_df, temp]).reset_index(drop = True)
  temp_df.to_csv(df_name)




# Combining all the DataFrame for a experiment
all_layer_df = pd.DataFrame(columns=['cycle', 'activation'])

for i in range(1, 6):
  file_name = f"layer_{i}.csv"
  temp_df = pd.read_csv(file_name, index_col = 0)
  all_layer_df = pd.concat([all_layer_df, temp_df]).reset_index(drop = True)




# Reading data for specific layer
specific_layer = pd.read_csv("layer_5.csv", index_col = 0)


# Splitting the whole data into train and test data
def train_test(df, split = 0.8):
  train = pd.DataFrame(columns = ['cycle', 'activation'])
  test = pd.DataFrame(columns = ['cycle', 'activation'])

  for af in activation_f:
    temp = df[df['activation'] == af]
    train_amount = int(temp.shape[0] * split)

    # Shuffling
    temp = temp.sample(frac = 1).reset_index(drop = True)

    temp_train = temp.iloc[ : train_amount, ]
    temp_test = temp.iloc[train_amount: , ]

    train = pd.concat([train, temp_train]).reset_index(drop = True)
    test = pd.concat([test, temp_test]).reset_index(drop = True)

  # Again, shuffling
  train = train.sample(frac = 1).reset_index(drop = True)
  test = test.sample(frac = 1).reset_index(drop = True)

  x_train, y_train, x_test, y_test = np.array(train['cycle']), np.array(train['activation']), np.array(test['cycle']), np.array(test['activation'])

  return x_train, y_train, x_test, y_test



x_train, y_train, x_test, y_test = train_test(specific_layer, split = 0.9)

# Modle description
clf = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', gamma = "auto"))
clf.fit(x_train.reshape(-1, 1), y_train)

# Prediction on test set
predictions = clf.predict(x_test.reshape(-1, 1))
print(classification_report(y_test, predictions))



# Getting mean from the Dataframe, for visualization
means_dict = {}
for i in range(1, 6):
  file_name = f"layer_{i}.csv"
  temp_df = pd.read_csv(file_name, index_col = 0)
  means = specific_layer.groupby('activation').mean().reset_index()
  for index, row in means.iterrows():
    key_name = f"{i}_{row['activation']}"
    means_dict[key_name] = row['cycle']

plt.bar(means_dict.keys(), means_dict.values())
plt.xticks(rotation=90)
plt.show()



# Model used in layer detection
clf = make_pipeline(
        StandardScaler(),
        LogisticRegression(
            random_state=42, multi_class = "ovr", n_jobs=16, verbose=1, solver = "saga"
        ),
    )

clf.fit(x_train.reshape(-1, 1), y_train)
predictions = clf.predict(x_test.reshape(-1, 1))
print(classification_report(y_test, predictions))




# Gaussian NB
gnb = GaussianNB()
predictions = gnb.fit(x_train.reshape(-1, 1), y_train).predict(x_test.reshape(-1, 1))
print(classification_report(y_test, predictions))





# Model ensembling
estimators = [('svc', GaussianNB())]
clf = StackingClassifier(
              estimators = estimators, 
              final_estimator = make_pipeline(
                                    StandardScaler(), 
                                    SVC(kernel = 'rbf', gamma = "auto"))
)
clf.fit(x_train.reshape(-1, 1), y_train)
predictions = clf.predict(x_test.reshape(-1, 1))
print(classification_report(y_test, predictions))
